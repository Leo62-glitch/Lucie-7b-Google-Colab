{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOYNG+sNKq/XuFYOLqf0XHj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torch transformers accelerate spaces"],"metadata":{"id":"Knb-3e2udCPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KfEQTy2c9ul"},"outputs":[],"source":["import spaces\n","import gradio as gr\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","from datetime import datetime\n","import os\n","\n","# Ton token est maintenant int√©gr√© directement\n","hf_token = \"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\"\n","\n","Title = \"\"\"# Welcome to üåüTonic's üå†Lucie-7B-Instruct Demo\"\"\"\n","\n","description = \"\"\"\n","üå†Lucie-7B-Instruct is a fine-tuned version of [Lucie-7B](https://huggingface.co/OpenLLM-France/Lucie-7B), an open-source, multilingual causal language model created by OpenLLM-France.\n","üå†Lucie-7B-Instruct is fine-tuned on synthetic instructions produced by ChatGPT and Gemma and a small set of customized prompts about OpenLLM and Lucie.\n","\"\"\"\n","\n","training = \"\"\"\n","## Training details\n","### Training data\n","Lucie-7B-Instruct is trained on the following datasets:\n","* [Alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) (English; 51604 samples)\n","* [Alpaca-cleaned-fr](https://huggingface.co/datasets/cmh/alpaca_data_cleaned_fr_52k) (French; 51655 samples)\n","* [Magpie-Gemma](https://huggingface.co/datasets/Magpie-Align/Magpie-Gemma2-Pro-200K-Filtered) (English; 195167 samples)\n","* [Wildchat](https://huggingface.co/datasets/allenai/WildChat-1M) (French subset; 26436 samples)\n","* Hard-coded prompts concerning OpenLLM and Lucie (based on [allenai/tulu-3-hard-coded-10x](https://huggingface.co/datasets/allenai/tulu-3-hard-coded-10x))\n","    * French: openllm_french.jsonl (24x10 samples)\n","    * English: openllm_english.jsonl (24x10 samples)\"\"\"\n","\n","join_us = \"\"\"\n","## Join us:\n","üåüTeamTonicüåü is always making cool demos! Join our active builder's üõ†Ô∏ècommunity üëª\n","[![Join us on Discord](https://img.shields.io/discord/1109943800132010065?label=Discord&logo=discord&style=flat-square)](https://discord.gg/qdfnvSPcqP)\n","On ü§óHuggingface: [MultiTransformer](https://huggingface.co/MultiTransformer)\n","On üåêGithub: [Tonic-AI](https://github.com/tonic-ai) & contribute toüåü [Build Tonic](https://git.tonic-ai.com/contribute)\n","ü§óBig thanks to Yuvi Sharma and all the folks at huggingface for the community grant ü§ó\n","\"\"\"\n","\n","# Initialize model and tokenizer\n","model_id = \"OpenLLM-France/Lucie-7B-Instruct\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Initialize tokenizer and model with token authentication\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    token=hf_token,\n","    trust_remote_code=True\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    token=hf_token,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n","    trust_remote_code=True\n",")\n","\n","config_json = model.config.to_dict()\n","\n","def format_model_info(config):\n","    info = []\n","    important_keys = [\n","        \"model_type\", \"vocab_size\", \"hidden_size\", \"num_attention_heads\",\n","        \"num_hidden_layers\", \"max_position_embeddings\", \"torch_dtype\"\n","    ]\n","    for key in important_keys:\n","        if key in config:\n","            value = config[key]\n","            # Convert torch_dtype to string representation if it exists\n","            if key == \"torch_dtype\" and hasattr(value, \"name\"):\n","                value = value.name\n","            info.append(f\"**{key}:** {value}\")\n","    return \"\\n\".join(info)\n","\n","@spaces.GPU\n","def generate_response(system_prompt, user_prompt, temperature, max_new_tokens, top_p, repetition_penalty, top_k):\n","    # Construct the full prompt with system and user messages\n","    full_prompt = f\"\"\"<|system|>{system_prompt}</s>\n","<|user|>{user_prompt}</s>\n","<|assistant|>\"\"\"\n","\n","    # Prepare the input prompt\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n","\n","    # Generate response\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        temperature=temperature,\n","        top_p=top_p,\n","        top_k=top_k,\n","        repetition_penalty=repetition_penalty,\n","        do_sample=True,\n","        pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    # Decode and return the response\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    # Extract only the assistant's response\n","    return response.split(\"<|assistant|>\")[-1].strip()\n","\n","# Create the Gradio interface\n","with gr.Blocks() as demo:\n","    gr.Markdown(Title)\n","\n","    with gr.Row():\n","        with gr.Column():\n","            gr.Markdown(description)\n","        with gr.Column():\n","            gr.Markdown(training)\n","\n","    with gr.Row():\n","        with gr.Column():\n","            with gr.Group():\n","                gr.Markdown(\"### Model Configuration\")\n","                gr.Markdown(format_model_info(config_json))\n","\n","        with gr.Column():\n","            with gr.Group():\n","                gr.Markdown(\"### Tokenizer Configuration\")\n","                gr.Markdown(f\"\"\"\n","                    **Vocabulary Size:** {tokenizer.vocab_size}\n","                    **Model Max Length:** {tokenizer.model_max_length}\n","                    **Padding Token:** {tokenizer.pad_token}\n","                    **EOS Token:** {tokenizer.eos_token}\n","                    \"\"\")\n","\n","    with gr.Row():\n","        with gr.Group():\n","            gr.Markdown(join_us)\n","\n","    with gr.Row():\n","        with gr.Column():\n","            # System prompt\n","            system_prompt = gr.Textbox(\n","                label=\"Message Syst√®me\",\n","                value=\"Tu es Lucie, une assistante IA fran√ßaise serviable et amicale. Tu r√©ponds toujours en fran√ßais de mani√®re pr√©cise et utile. Tu es honn√™te et si tu ne sais pas quelque chose, tu le dis simplement.\",\n","                lines=3\n","            )\n","\n","            # User prompt\n","            user_prompt = gr.Textbox(\n","                label=\"üó£Ô∏èVotre message\",\n","                placeholder=\"Entrez votre texte ici...\",\n","                lines=5\n","            )\n","\n","            with gr.Accordion(\"üß™Param√®tres avanc√©s\", open=False):\n","                temperature = gr.Slider(\n","                    minimum=0.1,\n","                    maximum=2.0,\n","                    value=0.7,\n","                    step=0.1,\n","                    label=\"üå°Ô∏èTemperature\"\n","                )\n","                max_new_tokens = gr.Slider(\n","                    minimum=1,\n","                    maximum=2048,\n","                    value=100,\n","                    step=1,\n","                    label=\"üí∂Longueur maximale\"\n","                )\n","                top_p = gr.Slider(\n","                    minimum=0.1,\n","                    maximum=1.0,\n","                    value=0.9,\n","                    step=0.1,\n","                    label=\"üèÖTop-p\"\n","                )\n","                top_k = gr.Slider(\n","                    minimum=1,\n","                    maximum=100,\n","                    value=50,\n","                    step=1,\n","                    label=\"üèÜTop-k\"\n","                )\n","                repetition_penalty = gr.Slider(\n","                    minimum=1.0,\n","                    maximum=2.0,\n","                    value=1.2,\n","                    step=0.1,\n","                    label=\"ü¶úP√©nalit√© de r√©p√©tition\"\n","                )\n","\n","            generate_btn = gr.Button(\"üå†G√©n√©rer\")\n","\n","        with gr.Column():\n","            # Output component\n","            output = gr.Textbox(\n","                label=\"üå†Lucie\",\n","                lines=10\n","            )\n","\n","    # Example prompts with all parameters\n","    gr.Examples(\n","        examples=[\n","            [\n","                \"Tu es Lucie, une assistante IA fran√ßaise serviable et amicale.\",\n","                \"Bonjour! Comment vas-tu aujourd'hui?\",\n","                0.7,  # temperature\n","                100,  # max_new_tokens\n","                0.9,  # top_p\n","                1.2,  # repetition_penalty\n","                50   # top_k\n","            ],\n","            [\n","                \"Tu es une experte en intelligence artificielle.\",\n","                \"Peux-tu m'expliquer ce qu'est l'intelligence artificielle?\",\n","                0.8,  # higher temperature for more creative explanation\n","                1024, # longer response\n","                0.95, # higher top_p for more diverse output\n","                1.1,  # lower repetition penalty\n","                40   # lower top_k for more focused output\n","            ],\n","            [\n","                \"Tu es une po√©tesse fran√ßaise.\",\n","                \"√âcris un court po√®me sur Paris.\",\n","                0.9,  # higher temperature for more creativity\n","                256,  # shorter for poetry\n","                0.95, # higher top_p for more creative language\n","                1.3,  # higher repetition penalty for unique words\n","                60   # higher top_k for more varied vocabulary\n","            ],\n","            [\n","                \"Tu es une experte en gastronomie fran√ßaise.\",\n","                \"Quels sont les plats traditionnels fran√ßais les plus connus?\",\n","                0.7,  # moderate temperature for factual response\n","                768,  # medium length\n","                0.9,  # balanced top_p\n","                1.2,  # standard repetition penalty\n","                50   # standard top_k\n","            ],\n","            [\n","                \"Tu es une historienne sp√©cialis√©e dans l'histoire de France.\",\n","                \"Explique-moi l'histoire de la R√©volution fran√ßaise en quelques phrases.\",\n","                0.6,  # lower temperature for more factual response\n","                1024, # longer for historical context\n","                0.85, # lower top_p for more focused output\n","                1.1,  # lower repetition penalty\n","                30   # lower top_k for more consistent output\n","            ]\n","        ],\n","        inputs=[\n","            system_prompt,\n","            user_prompt,\n","            temperature,\n","            max_new_tokens,\n","            top_p,\n","            repetition_penalty,\n","            top_k\n","        ],\n","        outputs=output,\n","        label=\"Exemples\"\n","    )\n","\n","    # Set up the generation event\n","    generate_btn.click(\n","        fn=generate_response,\n","        inputs=[system_prompt, user_prompt, temperature, max_new_tokens, top_p, repetition_penalty, top_k],\n","        outputs=output\n","    )\n","\n","# Launch the demo\n","if __name__ == \"__main__\":\n","    demo.launch(ssr_mode=False)\n"]}]}